\documentclass[english]{article}
\usepackage[latin1]{inputenc}
\makeatletter
\usepackage{babel}
\usepackage{natbib}
\usepackage{times}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{url}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{amsmath}
\graphicspath{{./images/}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\atantwo}{atan2}
\newcommand{\frametrans}[3]{{}_{#1}{#3}^{#2}}
\makeatother
\addtolength{\textwidth}{4cm}
\addtolength{\hoffset}{-2cm}
\begin{document}

\title{ITTIK}
\author{Olaf Booij}

\maketitle

The KITTI dataset is a standard for evaluating R\&D in autonomous
driving and mapping. An important part of the used sensor setup is a
surround LiDAR sensor which captures ranges and reflectivity of the
structures and objects around the vehicle.
This LiDAR data is not presented as raw sensor readings, which are hard
to interpret. Rather, the data is preprocessed into easy to comprehend
single scan 3D point clouds measured in a single 360 degrees rotation.
Generic point cloud software can be used to visualize and process such
data.

A drawback of the point clouds is that it is difficult to get the
precise time for each of the 3D point measurements. In addition, it is
difficult to compute the exact origin of the cast LiDAR rays. A more
general concern is that the point clouds do not reflect the dense and
structured form of the LiDAR measurement.

ITTIK tries to get back the raw LiDAR sensor data by undoing the steps
performed during the KITTI preprocessing. It does so using the KITTI
point clouds, as well as the available calibration data.

\section{Velodyne data}
% sensor
The LiDAR sensor used for the KITTI dataset is a Velodyne HDL-64E. This
device fires its 64 individual lasers with a frequency of 20 KHz, while
the device is rotating at a rate of 10 Hz, resulting in approximately
130,000 measurements per scan and 1.3 million measurements per
second. The 64 lasers do not fire at the same time. Instead, at each
timestep, two of the lasers fire in a fixed sequence. Because the device
keeps on rotating, each pair of points has a unique measurement time as
well as a unique device rotational position.

For each firing laser, the distance and reflectivity of the scene point
is measured. In addition, for each lower and upper block of 32 laser
fires the rotational position is measured. For each packet of 6 lower
and upper blocks, a timestamp is sent, which can be used to
synchronize the data with measurements from other sensors.

The rays of each laser have a different vertical and horizontal
direction. In addition, the lasers are distributed over 4 locations on
the device resulting in a slightly different offset of the ray origin.
The exact value of these directions and offsets are given in the
calibration stored on the device (more on this in the next section).
In Figure~\ref{fig:directions} the different directions of the laser
rays are shown. As can be seen the vertical view-angle is approximately
28 degrees and the horizontal view-angle 19 degrees.

\begin{figure}
    \centering
    %\input{directions.pdf_tex}
    \def\svgwidth{\columnwidth}
    \scalebox{0.9}{
    \def\svgwidth{.6 \columnwidth}
      \input{directions.pdf_tex}
    }
    \caption{Direction of each laser, plotted with there index, as
    indicated in the calibration file in degrees.}
		\label{fig:directions}
\end{figure}

It is important to understand the implications of this horizontal view
angle. Points from the same scan that are close to each other in the
scene, have a different measurement time. Take for example laser \#41
and laser \#46 (see Figure~\ref{fig:directions}). Vertically, they are
``neighbors'', but horizontally they are 19 degrees apart.

Thus, it is not easy to interpret the measurements of a single fire of all
lasers. It is like a depth camera where only 64 pixels are working spread over
the image plane. A point cloud representations is much more useful.

Still, for this point cloud we should consider that the device for the
example lasers has to rotate 19 degrees, before they measure the same
structure. Taking the rotation speed into account, two very close points
in the scene will have been measured with a time difference of
$\frac{19}{360} 100 \, \mbox{ms}= 5 \, \mbox{ms}$.

This time difference has to be taken into account of course when
processing the data. KITTI contains, in addition to the ``raw'' point
clouds, motion corrected data, in which the vehicle motion is
compensated for, for each laser. For moving objects, however, this is of
course more difficult.
% ... example of moving object in motion corrected data

This project is mainly meant to counter another
disadvantage of the point cloud representation. The spread of the
lasers, results in a non-uniform point cloud, hard to feed into standard image
processing tools such as convolutional neural networks. This is the main reason
to try to get the original Velodyne data back.
% ... example of piece of point cloud

An extra note on the amount of measurements and the timing of the lasers. In the
manual, it is stated that the device measures points at a rate of around 1.3 million
per second. However, when looking at the timing of the device each pair of
lasers is fired after $.73 \mu s$. If you would assume a monotonic firing behavior
of the device this would result in $\frac{2}{.73 \cdot 10^{-6}} \approx 2.74$ million
points per second. Later (Figure~\ref{fig:raw_meas}) we will indeed see that the
device has a bit peculiar firing behavior.

\section{Preprocessing}
\label{sec:preproc}
In order to easily interpret the Velodyne data, the raw byte stream is
preprocessed using a fixed process. This process has two parts. The first part, computes for
each laser fire from the laser id, the measured distance and measured
rotational position, the 3D point location. The second part, computes
for the raw intensity measurement, the surface reflectivity of the
measured point. We will (for now) only focus on the first part.

The preprocessing uses some parameters. Some of these are fixed for all
devices of the used LiDAR type, such as the location of the laser on the
device. There are, however, also device specific parameters, such as the
precise direction of the lasers, which are determined in the factory
using a specific calibration procedure and stored on the device. These
values are also transmitted by the device to be stored and used by the
preprocessing procedure.

For the KITTI dataset these calibration values are provided on the
website in the standard xml form:
\path{velodynecalib_S2_factory_flatness_intensity.xml}. The values in
this file are used in ITTIK, and for visualizations in this document
such as in Figure~\ref{fig:directions}.

We first thought this was the preprocessing algorithm described in the
User's Manual and Programming Guide. However, after implementing and
trying it out, this seemed not to be correct. For later reference, and
possible use for newer datasets, it is given in
Appendix~\ref{app:manualalg}. We found out by turning of some parts in
that quite complicated and hard to reverse algorithm, that the actual
algorithm used is much simpler. In addition, it does not use some of the
calibration parameters, which is actually a bit strange.

In the following, we describe the main part of the preprocessing
algorithm. Some trivial parts on rescaling the raw byte stream data is
left out.

The function to compute the 3D point $\mathbf{p}$, given a distance measurement $d$,
the rotational position $\theta$ and the laser id $l$ is close to
the formula:
\begin{eqnarray}
  \mathbf{p'} =
  f'(d, \theta, l, \Delta d) &=& \left(
           \begin{array}{l}
             (d + \Delta d) * \cos{\phi_l} * \sin(\theta - \Delta \theta_l) - h_l \cos(\theta - \Delta \theta_l) \\
             (d + \Delta d) * \cos{\phi_l} * \cos(\theta - \Delta \theta_l) + h_l \sin(\theta - \Delta \theta_l) \\
             (d + \Delta d) * \sin{\phi_l} + v_l
           \end{array}
         \right),
  \label{eq:ptilde}
\end{eqnarray}
in which the variables with subscript $l$ are the calibration parameters
specific to laser $l$ relative to the device. The values $\theta_l$ and
$\phi_l$ are the horizontal and vertical direction, named vertCorrection
and rotCorrection. The values $h_l$ and $b_l$ are the horizontal and
vertical position, named horizOffsetCorrection and vertOffsetCorrection.

It is close, but not complete. There is actually a non-symmetric adjustment
for the $x$ and $y$ component. They are based on parameters $\Delta
d_l$, $\Delta dx_l$ and $\Delta dy_l$, named distCorrection,
distCorrectionX and distCorrectionY, which are obtained using a factory
calibration procedure with objects placed at specific positions from the
sensor.  These parameters are used as follows to get $x$ and $y$
specific distance adjustments:
\begin{eqnarray}
  \Delta dx &=& \Delta dx_l + \frac{f_x'(d, \theta, l, \Delta d_l)(\Delta d_l - \Delta dx_l) - 2.40}{25.04 - 2.40}  \label{eq:ddx} \\
  \Delta dy &=& \Delta dy_l + \frac{f_y'(d, \theta, l, \Delta d_l)(\Delta d_l - \Delta dy_l) - 1.93}{25.04 - 1.93}, \label{eq:ddy}
\end{eqnarray}
with $f'(\cdot) = \left(f_x'(\cdot) , f_y'(\cdot) , f_z'(\cdot) \right)^T$.

These adjustment are then used for computing the actual point, with the
interesting note that the $z$ value is computed using the $y$-specific
distance adjustment:
\begin{eqnarray}
  \mathbf{p} =
  f(d, \theta, l) &=& \left(
           \begin{array}{l}
              f_x'(d, \theta, l, \Delta dx) \\
              f_y'(d, \theta, l, \Delta dy) \\
              f_z'(d, \theta, l, \Delta dy)
           \end{array}
         \right).
  \label{eq:p}
\end{eqnarray}

Apart from some rescaling that is left out, we also emitted some offset
from the process called ``pos'' in the User's Manual, which is not
explained further. This needs further investigation.

% Note, that the number of bytes used by the device to transmit its
% measurements is kept low. The storage of the (uncompressed) millimeter
% accurate point clouds is about 2 times larger than the format used by
% the sensor. 17+17+11+7 bit vs 16+8

\section{Reversing the preprocessing}
The aim is to reverse Equation~(\ref{eq:p}), getting back the distance
measurement $d$, the rotational position of the device $\theta$ and the
laser id $l$ given a point $p$.

The laser id can perhaps be computed by determining the vertical
direction of the point and matching it with the fixed $\phi_l$'s of each
laser. However, it is easier to use the order of the points in the KITTI
point clouds, which group the point per laser starting with the most
upward pointing laser. This approach is straightforward and described nicely
in~\cite{triess2020}. The order of the vertical angles $\phi_l$ described in calibration
can now be used to determine the laser id for each point.

Getting the other parameters is not that easy. We solve it in
an iterative fashion. First, an approximate value of both the distance and
the rotation is computed as initial values:
\begin{eqnarray}
  \theta &=& \atantwo(\mathbf{p}_x, \mathbf{p}_y) + \theta_l, \\
  d &=& ||p|| - \Delta d.
\end{eqnarray}
These values are then iteratively optimized. The rotational position is optimized using:
\begin{equation}
   \theta \leftarrow \atantwo\left(
                     \frac{\mathbf{p}_x + h_l * \cos(\theta - \Delta \theta_l)}{(d + \Delta dx) * \cos{\phi_l}},
                     \frac{\mathbf{p}_y - h_l * \sin(\theta - \Delta \theta_l)}{(d + \Delta dy) * \cos{\phi_l}}
           \right) + \Delta \theta_l,
   \label{eq:thetait}
\end{equation}
where $\Delta dx$ and $\Delta dy$ are calculated using
Equations~(\ref{eq:ptilde}) to~(\ref{eq:ddy}) again.

The distance measurement is optimized as:
\begin{equation}
    d \leftarrow \frac{p_x + h_l * \cos(\theta - \Delta \theta_l)}{\cos{\phi_l} * \sin(\theta - \Delta \theta_l)} - \Delta dx.  \label{eq:dita}
\end{equation}
Or, to increase numerical accuracy, if $p_y > p_x$:
\begin{equation}
    d \leftarrow \frac{p_y - h_l * \sin(\theta - \Delta \theta_l)}{\cos{\phi_l} * \cos(\theta - \Delta \theta_l)} - \Delta dy.  \label{eq:ditb}
\end{equation}

% makes the font a bit too small...:
%\begin{equation}
%    d \leftarrow \left\{ \begin{array}{rcl}
%    \frac{p_x + h_l * \cos(\theta - \Delta \theta_l)}{\cos{\phi_l} * \sin(\theta - \Delta \theta_l)} - \Delta dx & \mbox{for} &  p_y > p_x \\
%    \frac{p_y - h_l * \sin(\theta - \Delta \theta_l)}{\cos{\phi_l} * \cos(\theta - \Delta \theta_l)} - \Delta dy & \mbox{for} &  p_y =< p_x.
%    \end{array}\right.
%\end{equation}

The parameters $d$ and $\theta$ are refined using these equations until they
converge. This usually takes only 2 or 3 steps. In our implementation therefor
used a fixed number of 10 iterations (following well established decimal
tradition).

Note, that we also tried to formulate a closed form inverse of
Equation~(\ref{eq:p}). However, the solutions we found were not
numerically stable and given the limited accuracy of the point clouds
(millimeter accurate) the iterative approach seems more feasible.

\section{Results}
To assess if the reverse method described in the previous section works
correctly, we apply it on some of the KITTI data.

We first apply the algorithm on an arbitrary KITTI scan (see Figure~\ref{fig:example} to get the raw measurements and then
apply the standard preprocessing algorithm, as described in
Section~\ref{sec:preproc} to reconstruct back the 3D points. In theory these
reconstructed points should be the same as in the original scan. In practice, of
course, there is the quantification error, while the KITTI point clouds are
stored in millimeter precision.

\begin{figure}
  \begin{center}
    \subfigure[][LiDAR point cloud]
      {
       \includegraphics[width=.5\textwidth]{pointcloud.png}
      }
    \subfigure[][Image still]
      {
       \includegraphics[width=.4\textwidth]{image.jpg}
      }
      \caption{Example scan used in the results. This is scan number 71 and an
      image still from the raw dataset \texttt{2011\_09\_26\_drive\_0002}.}
  \label{fig:example}
  \end{center}
\end{figure}

The mean distance between the original 3D point and the reconstructed point for
the scan is 3.29 mm. In Figure~\ref{fig:norms_error}, this is split over the
different lasers sorted by vertical angle. It can be seen that lasers pointing
down have an average error up to 14 mm. This is much larger than expected given
the millimeter precision of the original points. This could also be related with
the distance of the points in the scene (the distance is on average smaller for
lasers pointing down).

\begin{figure}
    \centering
    %\input{directions.pdf_tex}
    \def\svgwidth{\columnwidth}
    \scalebox{0.9}{
    \def\svgwidth{.6 \columnwidth}
      \input{norms_error.pdf_tex}
    }
    \caption{Mean 3D distance between the original point and reconstructed point
    for each laser.}
		\label{fig:norms_error}
\end{figure}

The angle of the point in the horizontal plane of the sensor is the same for the
original and the reconstructed point. It is interesting to see that there is no
error here. It seems thus that the main error is in the vertical direction.  The
mean error of the distance of the point to the sensor is .85 mm.

% position alignment
Let us now look at the computed device rotational position per point.
Figure~\ref{subfig:raw_meas_measured} shows them for some of the points of the
used scan. As can be seen the computed rotation almost aligns for each of the
lasers. Still, there is a jitter visible up to 2 mrad. We do expect some
difference between the position of each point, due to the slightly different
timing of each laser, but this should be very regular and much smaller. In
Figure~\ref{subfig:raw_meas_simul} we simulated the expected rotations.

\section{Conclusion}
By reversing the Velodyne preprocessing, we can obtain the original measured
depth and rotational position. However, there still seems to be something wrong.
The cause is still unknown. It might be an error in the method, an error in the
calibration file, the emitted offset ``pos'', or a combination of these. Or, of
course, it could be just a silly bug...

%\begin{figure}
%    \centering
%    %\input{directions.pdf_tex}
%    \def\svgwidth{\columnwidth}
%    \scalebox{0.9}{
%    \def\svgwidth{.6 \columnwidth}
%      \input{raw_meas.pdf_tex}
%    }
%    \caption{Comparison of the rotational position measurements between the
%    different lasers as determined by reverse method for each point.
%    Lasers are ordered here by there ID (not by the vertical angle). The figure
%    only shows a limited range of 0 to 0.05 radians.}
%		\label{fig:raw_meas}
%\end{figure}

\begin{figure}
  \begin{center}
    \subfigure[][measured]
      {
        \def\svgwidth{\columnwidth}
        \scalebox{0.9}{
        \def\svgwidth{.5 \columnwidth}
          \input{raw_meas.pdf_tex}
        }
        \label{subfig:raw_meas_measured}
      }
    \subfigure[][expected]
      {
        \def\svgwidth{\columnwidth}
        \scalebox{0.9}{
        \def\svgwidth{.5 \columnwidth}
          \input{raw_meas_simul.pdf_tex}
        }
        \label{subfig:raw_meas_simul}
      }
    \caption{Rotational position of the device for the measured points of the
    different lasers.
    Lasers are ordered here by their ID (not by the vertical angle). The figures
    only show part of the scan in the range of 0 to 0.05 radians.
    \subref{subfig:raw_meas_measured} The computed rotations from the point
    cloud using the reverse method for each point.
    \subref{subfig:raw_meas_simul} The expected pattern of the rotations
    given the specs of the device.}
		\label{fig:raw_meas}
  \end{center}
\end{figure}

\appendix

\section*{Appendix A - Preprocessing according to the manual}
\label{app:manualalg}

The function to compute the 3D point $\mathbf{p}$, given a distance measurement $d$,
the rotational position $\theta$ and the laser id $l$ is close to
the formula:
\begin{eqnarray}
  \mathbf{p'} =
  f'(d, \theta, l, \Delta d) &=& \left(
           \begin{array}{l}
             (d + \Delta d) * \cos{\phi_l} * \sin(\theta - \Delta \theta_l) - h_l \cos(\theta - \Delta \theta_l) \\
             (d + \Delta d) * \cos{\phi_l} * \cos(\theta - \Delta \theta_l) + h_l \sin(\theta - \Delta \theta_l) \\
             (d + \Delta d) * \sin{\phi_l} + v_l
           \end{array}
         \right),
  \label{eq:ptilde}
\end{eqnarray}
in which the variables with subscript $l$ are the calibration parameters
specific to laser $l$ relative to the device. The values $\theta_l$ and
$\phi_l$ are the horizontal and vertical direction, named vertCorrection
and rotCorrection. The values $h_l$ and $b_l$ are the horizontal and
vertical position, named horizOffsetCorrection and vertOffsetCorrection.

It is close, but not complete. There is actually a non-symmetric adjustment
for the $x$ and $y$ component. They are based on parameters $\Delta
d_l$, $\Delta dx_l$ and $\Delta dy_l$, named distCorrection,
distCorrectionX and distCorrectionY, which are obtained using a factory
calibration procedure with objects placed at specific positions from the
sensor.  These parameters are used as follows to get $x$ and $y$
specific distance adjustments:
\begin{eqnarray}
  \Delta dx &=& \Delta dx_l + \frac{f_x'(d, \theta, l, \Delta d_l)(\Delta d_l - \Delta dx_l) - 2.40}{25.04 - 2.40}  \label{eq:ddx} \\
  \Delta dy &=& \Delta dy_l + \frac{f_y'(d, \theta, l, \Delta d_l)(\Delta d_l - \Delta dy_l) - 1.93}{25.04 - 1.93}, \label{eq:ddy}
\end{eqnarray}
with $f'(\cdot) = \left(f_x'(\cdot) , f_y'(\cdot) , f_z'(\cdot) \right)^T$.

These adjustment are then used for computing the actual point, with the
interesting note that the $z$ value is computed using the $y$-specific
distance adjustment:
\begin{eqnarray}
  \mathbf{p} =
  f(d, \theta, l) &=& \left(
           \begin{array}{l}
              f_x'(d, \theta, l, \Delta dx) \\
              f_y'(d, \theta, l, \Delta dy) \\
              f_z'(d, \theta, l, \Delta dy)
           \end{array}
         \right).
  \label{eq:p}
\end{eqnarray}

Apart from some rescaling that is left out, we also emitted some offset
from the process called ``pos'' in the User's Manual, which is not
explained further. This needs further investigation.

\bibliographystyle{apalike}
\bibliography{ittik.bib}

\end{document}


\begin{eqnarray}
  d_c &=& d + \Delta d_l \nonumber \\
  \theta &=& \theta_u - \theta_l \nonumber \\
  d' &=& d_c * \cos{\phi_l} \nonumber \\
  x &=& |d' * \sin{\theta} - h_l * \cos{\theta}| \nonumber \\
  y &=& |d' * \cos{\theta} + h_l * \sin{\theta}| \nonumber \\
  \Delta dx &=& (\Delta d_l - \Delta dx_l) * (x - 2.40) / (25.04 - 2.40) + \Delta dx_l \nonumber \\
  \Delta dy &=& (\Delta d_l - \Delta dy_l) * (y - 1.93) / (25.04 - 1.93) + \Delta dy_l \nonumber \\
  d_x &=& d_u + \Delta dx \nonumber \\
  d'_x &=& d_x * \cos{\phi_l} \nonumber \\
  d_y &=& d_u + \Delta dy \nonumber \\
  d'_y &=& d_y * \cos{\phi_l} \nonumber \\
  p &=&  \left(
           \begin{array}{c}
             d'_x * \sin{\theta} - h_l \cos{\theta} \nonumber \\
             d'_y * \cos{\theta} + h_l \sin{\theta} \nonumber \\
             d_y * \sin{\phi_l} + v_l
           \end{array}
         \right)
\end{eqnarray}


\section{Thingies to copy-paste}
\subsection{subbla}
\subsubsection{subsubbla}
\paragraph{}
\par{parbla}
\par


\begin{figure}
  \begin{center}
    \subfigure[][SubCaption1]
      {
       \label{subfig:name1}
       \includegraphics[width=\textwidth]{nogNiks}
      }
    \subfigure[][SubCaption2]
      {
       \label{subfig:name2}
       \includegraphics[height=8cm]{nogNiks}
      }
      \caption{Caption}
  \label{fig:name}
  \end{center}
\end{figure}
\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{nogNiks}
    %\scalebox{.5}{ \input{images/name.pstex_t} }
    \caption{Caption}
    \label{fig:name}
  \end{center}
\end{figure}
\begin{itemize}
\item
\item
\end{itemize}
\begin{description}
\item
\end{description}
\begin{enumerate}
\item
\end{enumerate}
\begin{equation}
  0 \neq 1
\end{equation}
\begin{eqnarray}
  0 &\neq& 1 \\
  A
         \left[
           \begin{array}{ccc}
             1 & 0 & 0 \\
             0 & 1 & 0 \\
             0 & 0 & 1
           \end{array}
         \right]
  A^T
  &=& B
\end{eqnarray}
\begin{table}
  %\renewcommand{\arraystretch}{1.2}
  \caption{}
  \label{tab:mane}
  \begin{center}
    \begin{tabular}{|c|c||c|}
      \hline
      \multicolumn{2}{|c||}{A} & B \\ \hline
       1 & 1 & 1 \\ \hline
       1 & 1 & 1 \\ \hline
    \end{tabular}
  \end{center}
\end{table}

